{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from batchgenerators.dataloading.data_loader import DataLoader\n",
    "from batchgenerators.dataloading import MultiThreadedAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode=\"train\", target_size=128):\n",
    "    tranform_list = []\n",
    "\n",
    "    if mode == \"train\":\n",
    "        tranform_list = [# CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=(target_size,target_size), order=1),\n",
    "                         MirrorTransform(axes=(1,)),\n",
    "                         ]\n",
    "\n",
    "\n",
    "    elif mode == \"val\":\n",
    "        tranform_list = [CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=target_size, order=1),\n",
    "                         ]\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        tranform_list = [CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=target_size, order=1),\n",
    "                         ]\n",
    "\n",
    "    tranform_list.append(NumpyToTensor())\n",
    "\n",
    "    return Compose(tranform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(root_dir,mode,keys,taskname):\n",
    "    image_names = keys\n",
    "    print(\"root_dir:\",root_dir,\"taskname : \",taskname,\"mode :\",mode,\"keys : \",keys )\n",
    "    dataDir = \"imagesTr\"\n",
    "    maskDir = \"masksTr\"\n",
    "    suffix =\".png\"  \n",
    "    img_data = []\n",
    "    img_labels = []\n",
    "\n",
    "    for image in image_names : \n",
    "        img = cv2.imread(osp.join(root_dir,taskname,dataDir,image+suffix))\n",
    "        print(\"image path: \",osp.join(root_dir,taskname,dataDir,image+suffix))\n",
    "        img_data.append(img)\n",
    "                \n",
    "        target_img = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        target_img_ = cv2.imread(osp.join(root_dir,taskname,maskDir,image+suffix),0)\n",
    "        target_img = np.maximum(target_img, target_img_)\n",
    "        img_labels.append(target_img)\n",
    "\n",
    "    return img_data,img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-138-403db30e36fd>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-138-403db30e36fd>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    data_loader = MedImageDataLoader(base_dir=base_dir,mode,batch_size,\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "class MedImageDataSet(object):\n",
    "\n",
    "        def __init__(self, base_dir, mode=\"train\",batch_size=16,num_batches=10000000,taskname=None,seed=None, num_processes=8,\n",
    "                     num_cached_per_queue=8 * 4, target_size=128, file_pattern='*.png', do_reshuffle=True, keys=None):\n",
    "\n",
    "            data_loader = MedImageDataLoader(base_dir=base_dir,mode,batch_size, \n",
    "                                        num_batches,taskname,seed,file_pattern,keys)\n",
    "\n",
    "            self.data_loader = data_loader\n",
    "            self.batch_size = batch_size\n",
    "            #self.do_reshuffle = do_reshuffle\n",
    "            self.number_of_slices = 1\n",
    "\n",
    "            self.transforms = get_transforms(mode=mode, target_size=target_size)\n",
    "            self.augmenter = MultiThreadedAugmenter(data_loader, self.transforms, num_processes=num_processes,num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "            self.augmenter.restart()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_loader)\n",
    "\n",
    "        def __iter__(self):\n",
    "            self.augmenter.renew()\n",
    "            return self.augmenter\n",
    "\n",
    "        def __next__(self):\n",
    "            return next(self.augmenter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedImageDataLoader(DataLoader):\n",
    "    def __init__(self, base_dir,mode ,batch_size,num_batches,taskname,seed=1234, file_pattern=\".png\",keys=None):\n",
    "        self.img_data ,self.img_labels = load_data_set(base_dir,mode,keys,taskname)\n",
    "        self.data = list(range(len(self.img_data)))\n",
    "\n",
    "        super().__init__(self.data, batch_size, seed_for_shuffle=seed)\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.indices = list(range(len(self.img_data))) \n",
    "        self.data_len = len(self.img_data)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        \n",
    "        idx = self.get_indices()\n",
    "        patients_for_batch = [self._data[i] for i in idx]\n",
    "        \n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for idx in patients_for_batch:\n",
    "            data.append(self.img_data[idx])\n",
    "            labels.append(self.img_labels[idx])\n",
    "            \n",
    "        return {'data': data, 'seg':seg}\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as TF\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trixi.util import Config\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    # Set your own path, if needed.\n",
    "    #data_root_dir = os.path.abspath('data')  # The path where the downloaded dataset is stored.\n",
    "    #data_root_dir = \"/home/ramesh/Desktop/WS/Implementation/experiment/Data/Filtereddataset\"\n",
    "    data_root_dir =\"/home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet\"\n",
    "    taskName = \"Task01_Hippocampus\"\n",
    "    #taskName = \"Task09_Spleen\"\n",
    "    c = Config(\n",
    "        update_from_argv=True,\n",
    "        # Train parameters\n",
    "        #num_classes=3,\n",
    "        #in_channels=1,\n",
    "        batch_size=2,\n",
    "        patch_size=64,\n",
    "        n_epochs=5,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cuda\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Segmentation_Experiment_Unet',\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,\n",
    "        start_visdom=True,\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        # Adapt to your own path, if needed.\n",
    "        #google_drive_id='1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C',\n",
    "        \n",
    "        dataset_name = taskName,\n",
    "        base_dir=os.path.abspath('output_experiment'),  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir=os.path.join(data_root_dir, taskName,'imagesTr'),  # This is where your training and validation data is stored\n",
    "       \n",
    "        #data_test_dir=os.path.join(data_root_dir, 'Task04_Hippocampus/preprocessed'),  # This is where your test data is stored\n",
    "        split_dir= os.path.join(data_root_dir, taskName,'preprocessed'),  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "    )\n",
    "\n",
    "    print(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"base_dir\": \"/home/ramesh/Desktop/IIITB/MedicalImaging/output_experiment\",\n",
      "    \"batch_size\": 2,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr\",\n",
      "    \"data_root_dir\": \"/home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet\",\n",
      "    \"dataset_name\": \"Task01_Hippocampus\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"n_epochs\": 5,\n",
      "    \"name\": \"Segmentation_Experiment_Unet\",\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"split_dir\": \"/home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/preprocessed\",\n",
      "    \"start_visdom\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl_dir:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/preprocessed\n",
      "tr_keys:  ['hippocampus_001_0020', 'hippocampus_001_0000', 'hippocampus_001_0019', 'hippocampus_001_0002', 'hippocampus_001_0001', 'hippocampus_001_0005', 'hippocampus_001_0014', 'hippocampus_001_0026', 'hippocampus_001_0033', 'hippocampus_001_0031', 'hippocampus_001_0018', 'hippocampus_001_0030', 'hippocampus_001_0010', 'hippocampus_001_0011', 'hippocampus_001_0007', 'hippocampus_001_0025', 'hippocampus_001_0024', 'hippocampus_001_0022', 'hippocampus_001_0003', 'hippocampus_001_0032', 'hippocampus_001_0028']\n",
      "val_keys:  ['hippocampus_001_0016', 'hippocampus_001_0013', 'hippocampus_001_0004', 'hippocampus_001_0027', 'hippocampus_001_0021', 'hippocampus_001_0017', 'hippocampus_001_0029', 'hippocampus_001_0015', 'hippocampus_001_0009', 'hippocampus_001_0008']\n",
      "test_keys:  ['hippocampus_001_0023', 'hippocampus_001_0006', 'hippocampus_001_0034']\n"
     ]
    }
   ],
   "source": [
    "pkl_dir = config.split_dir\n",
    "with open(os.path.join(pkl_dir, \"splits.pkl\"), 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "tr_keys = splits[config.fold]['train']\n",
    "val_keys = splits[config.fold]['val']\n",
    "test_keys = splits[config.fold]['test']\n",
    "print(\"pkl_dir: \",pkl_dir) \n",
    "print(\"tr_keys: \",tr_keys)\n",
    "print(\"val_keys: \",val_keys)\n",
    "print(\"test_keys: \",test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task01_Hippocampus'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = config.dataset_name\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_data_set(root_dir,taskname,mode=\"train\",keys=None):\n",
    "    image_names = keys\n",
    "\n",
    "    dataDir = \"imagesTr\"\n",
    "    maskDir = \"masksTr\"\n",
    "    suffix =\".png\"  \n",
    "    img_data = []\n",
    "    img_labels = []\n",
    "\n",
    "    for image in image_names : \n",
    "        img = cv2.imread(osp.join(root_dir,taskname,dataDir,image+suffix))\n",
    "        #print(\"image path: \",osp.join(root_dir,taskname,dataDir,image+suffix))\n",
    "        img_data.append(img)\n",
    "                \n",
    "        target_img = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        target_img_ = cv2.imread(osp.join(root_dir,taskname,maskDir,image+suffix),0)\n",
    "        target_img = np.maximum(target_img, target_img_)\n",
    "        img_labels.append(target_img)\n",
    "\n",
    "    return img_data,img_labels\n",
    "\n",
    "class NucleusDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None, target_transform=None, mode =\"train\",\n",
    "                  do_reshuffle=True, keys=None,taskname = None,batch_size=16, num_batches=10000000, seed=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        self.taskname = taskname\n",
    "        self.image_names = keys\n",
    "        self.mode = mode\n",
    "        self.data_len = len(self.image_names)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        dataDir = \"imagesTr\"\n",
    "        maskDir = \"masksTr\"\n",
    "        suffix =\".png\" \n",
    "        print(\"root_dir :\",root_dir, \" taskname : \",taskname,\"self.mode :\",self.mode)\n",
    "        print(\" path : \",osp.join(self.root_dir, taskname))\n",
    "        \n",
    "        if not self._check_task_exists():\n",
    "            raise RuntimeError(\"Task does not exist\")\n",
    "            \n",
    "\n",
    "        if self.mode==\"train\":\n",
    "\n",
    "            print(\" Mode : \",mode , \" train image_names :\",self.image_names)\n",
    "            self.train_data ,self.train_labels =   load_data_set(root_dir=self.root_dir,taskname=self.taskname,mode=self.mode,keys=self.image_names)\n",
    "            \n",
    "        elif self.mode ==\"val\":\n",
    "\n",
    "            print(\" Mode : \",mode , \" val image_names :\",self.image_names)\n",
    "            self.val_data ,self.val_labels = load_data_set(root_dir=self.root_dir,taskname=self.taskname,mode=self.mode,keys=self.image_names)\n",
    "\n",
    "        else :\n",
    "\n",
    "            print(\" Mode : \",mode , \" test image_names :\",self.image_names)\n",
    "            self.test_data ,self.test_labels = load_data_set(root_dir=self.root_dir,taskname=self.taskname,mode=self.mode,keys=self.image_names)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.mode==\"train\":\n",
    "            image, mask = self.train_data[item], self.train_labels[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            return image, mask\n",
    "                                              \n",
    "        elif self.mode==\"val\":\n",
    "            image, mask = self.val_data[item], self.val_labels[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            return image, mask     \n",
    "                                              \n",
    "        else:\n",
    "            image, mask = self.test_data[item], self.test_labels[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            return image, mask     \n",
    "\n",
    "    def _check_exists(self):\n",
    "        return osp.exists(osp.join(self.root_dir, \"train\")) and osp.exists(osp.join(self.root_dir, \"test\"))\n",
    "    \n",
    "    def _check_task_exists(self):\n",
    "        return osp.exists(osp.join(self.root_dir, self.taskname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy(tensor):\n",
    "    t_numpy = tensor.cpu().numpy()\n",
    "    t_numpy = np.transpose(t_numpy, [0, 2, 3, 1])\n",
    "    t_numpy = np.squeeze(t_numpy)\n",
    "\n",
    "    return t_numpy\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        elif len(data.shape) == 3:\n",
    "            data = data.transpose((2, 0, 1))\n",
    "        else:\n",
    "            print(\"Unsupported shape!\")\n",
    "        return torch.from_numpy(data)\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, image):\n",
    "        image = image.astype(np.float32) / 255\n",
    "        return image\n",
    "\n",
    "class Horizontal_flip:\n",
    "    def __call__(self, image):\n",
    "        # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "        image = image[:, ::-1]\n",
    "        return image\n",
    "\n",
    "class Rescale:\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return cv2.resize(image, (self.output_size, self.output_size), cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir : /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet  taskname :  Task01_Hippocampus self.mode : train\n",
      " path :  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus\n",
      " Mode :  train  train image_names : ['hippocampus_001_0020', 'hippocampus_001_0000', 'hippocampus_001_0019', 'hippocampus_001_0002', 'hippocampus_001_0001', 'hippocampus_001_0005', 'hippocampus_001_0014', 'hippocampus_001_0026', 'hippocampus_001_0033', 'hippocampus_001_0031', 'hippocampus_001_0018', 'hippocampus_001_0030', 'hippocampus_001_0010', 'hippocampus_001_0011', 'hippocampus_001_0007', 'hippocampus_001_0025', 'hippocampus_001_0024', 'hippocampus_001_0022', 'hippocampus_001_0003', 'hippocampus_001_0032', 'hippocampus_001_0028']\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        NucleusDataset(config.data_root_dir, train=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           Rescale(config.patch_size),\n",
    "                           ToTensor()\n",
    "                       ]),\n",
    "                       target_transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           Rescale(config.patch_size),\n",
    "                           ToTensor()\n",
    "                       ]),\n",
    "                      mode =\"train\",\n",
    "                      keys = tr_keys,\n",
    "                      taskname = task),\n",
    "        batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet taskname :  Task01_Hippocampus mode : train keys :  ['hippocampus_001_0020', 'hippocampus_001_0000', 'hippocampus_001_0019', 'hippocampus_001_0002', 'hippocampus_001_0001', 'hippocampus_001_0005', 'hippocampus_001_0014', 'hippocampus_001_0026', 'hippocampus_001_0033', 'hippocampus_001_0031', 'hippocampus_001_0018', 'hippocampus_001_0030', 'hippocampus_001_0010', 'hippocampus_001_0011', 'hippocampus_001_0007', 'hippocampus_001_0025', 'hippocampus_001_0024', 'hippocampus_001_0022', 'hippocampus_001_0003', 'hippocampus_001_0032', 'hippocampus_001_0028']\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0020.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0000.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0019.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0002.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0001.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0005.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0014.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0026.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0033.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0031.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0018.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0030.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0010.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0011.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0007.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0025.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0024.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0022.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0003.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0032.png\n",
      "image path:  /home/ramesh/Desktop/IIITB/experiment/data/FilteredDataSet/Task01_Hippocampus/imagesTr/hippocampus_001_0028.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'seed_for_shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-a32ea1d794c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m tr_data_loader = MedImageDataSet(base_dir=config.data_root_dir, mode=\"train\", batch_size=4, num_batches=10000000, taskname = config.dataset_name,seed=None, \n\u001b[0;32m----> 2\u001b[0;31m                                 num_processes=8,num_cached_per_queue=8 * 4, target_size=128, file_pattern='*.png', do_reshuffle=True, keys=tr_keys)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-220d89e9c1ff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_dir, mode, batch_size, num_batches, taskname, seed, num_processes, num_cached_per_queue, target_size, file_pattern, do_reshuffle, keys)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             data_loader = MedImageDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, \n\u001b[0;32m----> 7\u001b[0;31m                                         num_batches=num_batches, taskname=taskname,seed=seed,file_pattern=file_pattern,keys=keys)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-148-98108cb6c82c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_dir, mode, batch_size, num_batches, taskname, seed, file_pattern, keys)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_for_shuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'seed_for_shuffle'"
     ]
    }
   ],
   "source": [
    "tr_data_loader = MedImageDataSet(base_dir=config.data_root_dir, mode=\"train\", batch_size=4, num_batches=10000000, taskname = config.dataset_name,seed=None, \n",
    "                                num_processes=8,num_cached_per_queue=8 * 4, target_size=128, file_pattern='*.png', do_reshuffle=True, keys=tr_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-121-80f469ac8712>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-121-80f469ac8712>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    taskname=None,seed=None, num_processes=8,num_cached_per_queue=8 * 4, target_size=128, file_pattern='*.png', do_reshuffle=True, keys=tr_keys):\u001b[0m\n\u001b[0m                                                                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "base_dir, mode=\"train\",batch_size=16,num_batches=10000000,taskname=None,seed=None, num_processes=8,num_cached_per_queue=8 * 4, target_size=128, file_pattern='*.png', do_reshuffle=True, keys=tr_keys):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
