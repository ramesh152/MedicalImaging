{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#from data import NucleusDataset, Rescale, ToTensor, Normalize\n",
    "#from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        elif len(data.shape) == 3:\n",
    "            data = data.transpose((2, 0, 1))\n",
    "        else:\n",
    "            print(\"Unsupported shape!\")\n",
    "        return torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize_old:\n",
    "    def __call__(self, image):\n",
    "        image = image.astype(np.float32) / 255\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __call__(self, image):\n",
    "        image = image.astype(np.float32)\n",
    "        mean = np.mean(image)  # mean for data centering\n",
    "        std = np.std(image)  # std for data normalization\n",
    "\n",
    "        image -= mean\n",
    "        image /= std\n",
    "        #Normalization of the train set\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale:\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return cv2.resize(image, (self.output_size, self.output_size), cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, kernel_size=3, padding=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv5_t = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "\n",
    "        self.conv6_1 = nn.Conv2d(512, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv6_2 = nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv6_t = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "\n",
    "        self.conv7_1 = nn.Conv2d(256, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv7_2 = nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv7_t = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(128, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv8_2 = nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv8_t = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(64, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv9_2 = nn.Conv2d(32, 32, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, 1, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = F.relu(self.conv1_1(x))\n",
    "        conv1 = F.relu(self.conv1_2(conv1))\n",
    "        pool1 = self.maxpool1(conv1)\n",
    "\n",
    "        conv2 = F.relu(self.conv2_1(pool1))\n",
    "        conv2 = F.relu(self.conv2_2(conv2))\n",
    "        pool2 = self.maxpool2(conv2)\n",
    "\n",
    "        conv3 = F.relu(self.conv3_1(pool2))\n",
    "        conv3 = F.relu(self.conv3_2(conv3))\n",
    "        pool3 = self.maxpool3(conv3)\n",
    "\n",
    "        conv4 = F.relu(self.conv4_1(pool3))\n",
    "        conv4 = F.relu(self.conv4_2(conv4))\n",
    "        pool4 = self.maxpool4(conv4)\n",
    "\n",
    "        conv5 = F.relu(self.conv5_1(pool4))\n",
    "        conv5 = F.relu(self.conv5_2(conv5))\n",
    "\n",
    "        up6 = torch.cat((self.conv5_t(conv5), conv4), dim=1)\n",
    "        conv6 = F.relu(self.conv6_1(up6))\n",
    "        conv6 = F.relu(self.conv6_2(conv6))\n",
    "\n",
    "        up7 = torch.cat((self.conv6_t(conv6), conv3), dim=1)\n",
    "        conv7 = F.relu(self.conv7_1(up7))\n",
    "        conv7 = F.relu(self.conv7_2(conv7))\n",
    "\n",
    "        up8 = torch.cat((self.conv7_t(conv7), conv2), dim=1)\n",
    "        conv8 = F.relu(self.conv8_1(up8))\n",
    "        conv8 = F.relu(self.conv8_2(conv8))\n",
    "\n",
    "        up9 = torch.cat((self.conv8_t(conv8), conv1), dim=1)\n",
    "        conv9 = F.relu(self.conv9_1(up9))\n",
    "        conv9 = F.relu(self.conv9_2(conv9))\n",
    "        \n",
    "        return torch.sigmoid(self.conv10(conv9))\n",
    "        #return F.sigmoid(self.conv10(conv9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (conv1_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_t): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv6_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_t): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv7_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7_t): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv8_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8_t): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv9_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_temp = UNet()\n",
    "print(model_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleusDatasetOld(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found.\")\n",
    "\n",
    "        if self.train:\n",
    "            print(\"root_dir :\",str(root_dir))\n",
    "            #self.image_names = os.listdir(osp.join(self.root_dir, \"train\"))\n",
    "            self.image_names = os.listdir(os.path.join(self.root_dir, datafldr))\n",
    "            print(\"train image_names :\",self.image_names)\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "\n",
    "            for image_name in tqdm(self.image_names):\n",
    "                train_img = cv2.imread(osp.join(self.root_dir, datafldr, image_name, \"images\", image_name + \".png\"))\n",
    "                #train_img = cv2.imread(osp.join(self.root_dir, \"train\",image_name))\n",
    "\n",
    "                self.train_data.append(train_img)\n",
    "\n",
    "                target_img = np.zeros(train_img.shape[:2], dtype=np.uint8)\n",
    "                for target in glob(osp.join(self.root_dir, \"train\", image_name, \"masks\", \"*.png\")):\n",
    "                    target_img_ = cv2.imread(target, 0)\n",
    "                    target_img = np.maximum(target_img, target_img_)\n",
    "\n",
    "                self.train_labels.append(target_img)\n",
    "        else:\n",
    "            self.image_names = os.listdir(osp.join(self.root_dir, \"test\"))\n",
    "            self.test_data = []\n",
    "            #print(\"test image_names :\",self.image_names)\n",
    "\n",
    "            for image_name in tqdm(self.image_names):\n",
    "                test_img = cv2.imread(osp.join(self.root_dir, \"test\", image_name, \"images\", image_name + \".png\"))\n",
    "                #test_img = cv2.imread(osp.join(self.root_dir, \"test\", image_name))  \n",
    "\n",
    "                self.test_data.append(test_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            image, mask = self.train_data[item], self.train_labels[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            return image, mask\n",
    "        else:\n",
    "            image = self.test_data[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return osp.exists(osp.join(self.root_dir, \"train\")) and osp.exists(osp.join(self.root_dir, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleusDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        dataDir = \"imagesTr\"\n",
    "        maskDir = \"masksTr\"\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found.\")\n",
    "\n",
    "        if self.train:\n",
    "            print(\"root_dir :\",str(root_dir))\n",
    "            self.image_names = os.listdir(os.path.join(self.root_dir, \"train\",dataDir))\n",
    "            #print(\"train image_names :\",self.image_names)\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            for image in self.image_names: \n",
    "                train_img = cv2.imread(osp.join(self.root_dir,\"train\",dataDir,image))\n",
    "                self.train_data.append(train_img)\n",
    "                \n",
    "                target_img = np.zeros(train_img.shape[:2], dtype=np.uint8)\n",
    "                target_img_ = cv2.imread(osp.join(self.root_dir,\"train\",maskDir,image),0)\n",
    "                target_img = np.maximum(target_img, target_img_)\n",
    "                self.train_labels.append(target_img)\n",
    "        else:\n",
    "            self.image_names = os.listdir(osp.join(self.root_dir, \"test\",dataDir))\n",
    "            self.test_data = []\n",
    "            for image in self.image_names:  \n",
    "                #print(\"test image_names :\",self.image_names)\n",
    "                test_img = cv2.imread(osp.join(self.root_dir, \"test\", dataDir,image))\n",
    "                self.test_data.append(test_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            image, mask = self.train_data[item], self.train_labels[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            if self.target_transform:\n",
    "                mask = self.target_transform(mask)\n",
    "\n",
    "            return image, mask\n",
    "        else:\n",
    "            image = self.test_data[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return osp.exists(osp.join(self.root_dir, \"train\")) and osp.exists(osp.join(self.root_dir, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(true, logits, eps=1e-7):\n",
    "    \"\"\"Computes the Sørensen–Dice loss.\n",
    "    Note that PyTorch optimizers minimize a loss. In this\n",
    "    case, we would like to maximize the dice loss so we\n",
    "    return the negated dice loss.\n",
    "    Args:\n",
    "        true: a tensor of shape [B, 1, H, W].\n",
    "        logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
    "            the raw output or logits of the model.\n",
    "        eps: added to the denominator for numerical stability.\n",
    "    Returns:\n",
    "        dice_loss: the Sørensen–Dice loss.\n",
    "    \"\"\"\n",
    "    num_classes = logits.shape[1]\n",
    "    if num_classes == 1:\n",
    "        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
    "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
    "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "        pos_prob = torch.sigmoid(logits)\n",
    "        neg_prob = 1 - pos_prob\n",
    "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
    "    else:\n",
    "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
    "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "    true_1_hot = true_1_hot.type(logits.type())\n",
    "    dims = (0,) + tuple(range(2, true.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "    return (1 - dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size, learning_rate,root_dir):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        NucleusDataset(root_dir, train=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           Rescale(_imageSize),\n",
    "                           ToTensor()\n",
    "                       ]),\n",
    "                       target_transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           Rescale(_imageSize),\n",
    "                           ToTensor()\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            #print(\"images shape :\",images.shape ,\"masks shape:\",masks.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            #print(\"output shape :\",images.shape ,\"input images shape : \", images.shape)\n",
    "            bce = F.binary_cross_entropy(output, masks)\n",
    "            #dice_coefficient = dice_loss(masks, output)\n",
    "            loss = bce \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(\"Loss: {:.4f}\\n\".format(epoch_loss))\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(model, os.path.join(model_dir,\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root_dir =\"E:\\IIITB\\Documents\\data\\Dataset\\Liver\"\n",
    "model_dir = \"E:\\IIITB\\Documents\\data\\Dataset\\Liver\\model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_figSize = 80\n",
    "_fontSize = 60\n",
    "_imageSize= 512\n",
    "_learningRate = 0.0001\n",
    "_batchSize = 2\n",
    "_epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir : E:\\IIITB\\Documents\\data\\Dataset\\Liver\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sukanya\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sukanya\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type UNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "train(epochs=_epoch,\n",
    "          batch_size=_batchSize,\n",
    "          learning_rate=_learningRate,root_dir=img_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_old(images, masks, columns=6):\n",
    "    fig = plt.figure()\n",
    "    rows = np.ceil((images.shape[0] + masks.shape[0]) / columns)\n",
    "    index = 1\n",
    "    for image, mask in zip(images, masks):\n",
    "        f1 = fig.add_subplot(rows, columns, index)\n",
    "        f1.set_title('input')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        index += 1\n",
    "\n",
    "        f2 = fig.add_subplot(rows, columns, index)\n",
    "        f2.set_title('prediction')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(mask)\n",
    "        index += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, masks, columns=4):\n",
    "    fig = plt.figure(figsize=(_figSize,_figSize))\n",
    "    rows = np.ceil((images.shape[0] + masks.shape[0]) / columns)\n",
    "    index = 1\n",
    "    for image, mask in zip(images, masks):\n",
    "        f1 = fig.add_subplot(rows, columns, index)\n",
    "        f1.set_title('input', fontsize = _fontSize)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image,cmap = \"gray\")\n",
    "        index += 1\n",
    "\n",
    "        f2 = fig.add_subplot(rows, columns, index)\n",
    "        f2.set_title('prediction',fontsize = _fontSize)\n",
    "        plt.axis('off')\n",
    "        #plt.imshow(mask.float.cpu(),cmap = \"gray\")\n",
    "        plt.imshow(torch.argmax(masks.cpu(), dim=1, keepdim=True))\n",
    "        index += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_numpy(tensor):\n",
    "    t_numpy = tensor.cpu().numpy()\n",
    "    t_numpy = np.transpose(t_numpy, [0, 2, 3, 1])\n",
    "    t_numpy = np.squeeze(t_numpy)\n",
    "\n",
    "    return t_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    #test_img_root_dir =\"D:/MedicalImageProcessing/MedicalImaging/Nuclei-segmentation-master_mod/Nuclei-segmentation-master/data\"\n",
    "    test_img_root_dir =\"E:\\IIITB\\Documents\\data\\Dataset\\Liver\"\n",
    "    test_loader = DataLoader(NucleusDataset(test_img_root_dir,\n",
    "                                            train=False,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                Normalize(),\n",
    "                                                Rescale(_imageSize),\n",
    "                                                ToTensor()])),\n",
    "                             batch_size=6,\n",
    "                             shuffle=True)\n",
    "\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    #model = torch.load(\"models/model.pt\")\n",
    "    model = torch.load(os.path.join(model_dir,\"model.pt\"))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = next(iter(test_loader)).to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        images = tensor_to_numpy(images)\n",
    "        outputs = tensor_to_numpy(outputs)\n",
    "        print(\"Final images shape \",images.shape,\"outputs shape : \",outputs.shape) \n",
    "        show_images(images, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 5GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-42ea2ed48c94>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-883b8afb4f6a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mup9\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv8_t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mconv9\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv9_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mup9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mconv9\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv9_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 5GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader = torch.utils.data.DataLoader(\n",
    "        NucleusDataset(root_dir, train=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           #Rescale(_imageSize),\n",
    "                           ToTensor()\n",
    "                       ]),\n",
    "                       target_transform=transforms.Compose([\n",
    "                           Normalize(),\n",
    "                           #Rescale(_imageSize),\n",
    "                           ToTensor()\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
