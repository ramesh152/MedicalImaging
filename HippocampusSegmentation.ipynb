{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "from trixi.util import Config\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = \"D:/MedicalImageProcessing/Thesis_Practice/basic_unet_example/data/\"# os.path.abspath('data')  # The path where the downloaded dataset is stored.\n",
    "\n",
    "c = Config(\n",
    "        update_from_argv=True,\n",
    "\n",
    "        # Train parameters\n",
    "        num_classes=3,\n",
    "        in_channels=1,\n",
    "        batch_size=8,\n",
    "        patch_size=64,\n",
    "        n_epochs=1,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cpu\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Basic_Unet',\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,\n",
    "        start_visdom=True,\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        # Adapt to your own path, if needed.\n",
    "        google_drive_id='1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C',\n",
    "        dataset_name='Task04_Hippocampus',\n",
    "        base_dir=os.path.abspath('output_experiment'),  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir=os.path.join(data_root_dir, 'Task04_Hippocampus/preprocessed'),  # This is where your training and validation data is stored\n",
    "        data_test_dir=os.path.join(data_root_dir, 'Task04_Hippocampus/preprocessed'),  # This is where your test data is stored\n",
    "\n",
    "        split_dir=os.path.join(data_root_dir, 'Task04_Hippocampus'),  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., apply_nonlin=None, batch_dice=False, do_bg=True, smooth_in_nom=True, background_weight=1, rebalance_weights=None):\n",
    "        \"\"\"\n",
    "        hahaa no documentation for you today\n",
    "        :param smooth:\n",
    "        :param apply_nonlin:\n",
    "        :param batch_dice:\n",
    "        :param do_bg:\n",
    "        :param smooth_in_nom:\n",
    "        :param background_weight:\n",
    "        :param rebalance_weights:\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        if not do_bg:\n",
    "            assert background_weight == 1, \"if there is no bg, then set background weight to 1 you dummy\"\n",
    "        self.rebalance_weights = rebalance_weights\n",
    "        self.background_weight = background_weight\n",
    "        if smooth_in_nom:\n",
    "            self.smooth_in_nom = smooth\n",
    "        else:\n",
    "            self.smooth_in_nom = 0\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.y_onehot = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "        shp_x = x.shape\n",
    "        shp_y = y.shape\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            y = y.view((shp_y[0], 1, *shp_y[1:]))\n",
    "        # now x and y should have shape (B, C, X, Y(, Z))) and (B, 1, X, Y(, Z))), respectively\n",
    "        y_onehot = torch.zeros(shp_x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            y_onehot = y_onehot.cuda(x.device.index)\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "        if not self.do_bg:\n",
    "            x = x[:, 1:]\n",
    "            y_onehot = y_onehot[:, 1:]\n",
    "        if not self.batch_dice:\n",
    "            if self.background_weight != 1 or (self.rebalance_weights is not None):\n",
    "                raise NotImplementedError(\"nah son\")\n",
    "            l = soft_dice(x, y_onehot, self.smooth, self.smooth_in_nom)\n",
    "        else:\n",
    "            l = soft_dice_per_batch_2(x, y_onehot, self.smooth, self.smooth_in_nom,\n",
    "                                      background_weight=self.background_weight,\n",
    "                                      rebalance_weights=self.rebalance_weights)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, in_channels=1, initial_filter_size=64, kernel_size=3, num_downs=4, norm_layer=nn.InstanceNorm2d):\n",
    "        # norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-1), out_channels=initial_filter_size * 2 ** num_downs,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, norm_layer=norm_layer, innermost=True)\n",
    "        for i in range(1, num_downs):\n",
    "            unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-(i+1)),\n",
    "                                                 out_channels=initial_filter_size * 2 ** (num_downs-i),\n",
    "                                                 num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=in_channels, out_channels=initial_filter_size,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer,\n",
    "                                             outermost=True)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None, num_classes=1, kernel_size=3,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        # downconv\n",
    "        pool = nn.MaxPool2d(2, stride=2)\n",
    "        conv1 = self.contract(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "        conv2 = self.contract(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "\n",
    "        # upconv\n",
    "        conv3 = self.expand(in_channels=out_channels*2, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        conv4 = self.expand(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        if outermost:\n",
    "            final = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
    "            down = [conv1, conv2]\n",
    "            up = [conv3, conv4, final]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels,\n",
    "                                        kernel_size=2, stride=2)\n",
    "            model = [pool, conv1, conv2, upconv]\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels, kernel_size=2, stride=2)\n",
    "\n",
    "            down = [pool, conv1, conv2]\n",
    "            up = [conv3, conv4, upconv]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    @staticmethod\n",
    "    def contract(in_channels, out_channels, kernel_size=3, norm_layer=nn.InstanceNorm2d):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            norm_layer(out_channels),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def expand(in_channels, out_channels, kernel_size=3):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(layer, target_width, target_height):\n",
    "        batch_size, n_channels, layer_width, layer_height = layer.size()\n",
    "        xy1 = (layer_width - target_width) // 2\n",
    "        xy2 = (layer_height - target_height) // 2\n",
    "        return layer[:, :, xy1:(xy1 + target_width), xy2:(xy2 + target_height)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            crop = self.center_crop(self.model(x), x.size()[2], x.size()[3])\n",
    "            return torch.cat([x, crop], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetExperiment():\n",
    "    \"\"\"\n",
    "    The UnetExperiment is inherited from the PytorchExperiment. It implements the basic life cycle for a segmentation task with UNet(https://arxiv.org/abs/1505.04597).\n",
    "    It is optimized to work with the provided NumpyDataLoader.\n",
    "\n",
    "    The basic life cycle of a UnetExperiment is the same s PytorchExperiment:\n",
    "\n",
    "        setup()\n",
    "        (--> Automatically restore values if a previous checkpoint is given)\n",
    "        prepare()\n",
    "\n",
    "        for epoch in n_epochs:\n",
    "            train()\n",
    "            validate()\n",
    "            (--> save current checkpoint)\n",
    "\n",
    "        end()\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        pkl_dir = self.config.split_dir\n",
    "        with open(os.path.join(pkl_dir, \"splits.pkl\"), 'rb') as f:\n",
    "            splits = pickle.load(f)\n",
    "\n",
    "        tr_keys = splits[self.config.fold]['train']\n",
    "        val_keys = splits[self.config.fold]['val']\n",
    "        test_keys = splits[self.config.fold]['test']\n",
    "\n",
    "        self.device = torch.device(self.config.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.train_data_loader = NumpyDataSet(self.config.data_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                              keys=tr_keys)\n",
    "        self.val_data_loader = NumpyDataSet(self.config.data_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                            keys=val_keys, mode=\"val\", do_reshuffle=False)\n",
    "        self.test_data_loader = NumpyDataSet(self.config.data_test_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                             keys=test_keys, mode=\"test\", do_reshuffle=False)\n",
    "        self.model = UNet(num_classes=self.config.num_classes, in_channels=self.config.in_channels)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # We use a combination of DICE-loss and CE-Loss in this example.\n",
    "        # This proved good in the medical segmentation decathlon.\n",
    "        self.dice_loss = SoftDiceLoss(batch_dice=True)  # Softmax for DICE Loss!\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()  # No softmax for CE Loss -> is implemented in torch!\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # If directory for checkpoint is provided, we load it.\n",
    "        if self.config.do_load_checkpoint:\n",
    "            if self.config.checkpoint_dir == '':\n",
    "                print('checkpoint_dir is empty, please provide directory to load checkpoint.')\n",
    "            else:\n",
    "                self.load_checkpoint(name=self.config.checkpoint_dir, save_types=(\"model\"))\n",
    "\n",
    "        self.save_checkpoint(name=\"checkpoint_start\")\n",
    "        self.elog.print('Experiment set up.')\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.elog.print('=====TRAIN=====')\n",
    "        self.model.train()\n",
    "\n",
    "        data = None\n",
    "        batch_counter = 0\n",
    "        for data_batch in self.train_data_loader:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Shape of data_batch = [1, b, c, w, h]\n",
    "            # Desired shape = [b, c, w, h]\n",
    "            # Move data and target to the GPU\n",
    "            data = data_batch['data'][0].float().to(self.device)\n",
    "            target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "            pred = self.model(data)\n",
    "            pred_softmax = F.softmax(pred, dim=1)  # We calculate a softmax, because our SoftDiceLoss expects that as an input. The CE-Loss does the softmax internally.\n",
    "\n",
    "            loss = self.dice_loss(pred_softmax, target.squeeze()) + self.ce_loss(pred, target.squeeze())\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Some logging and plotting\n",
    "            if (batch_counter % self.config.plot_freq) == 0:\n",
    "                self.elog.print('Epoch: {0} Loss: {1:.4f}'.format(self._epoch_idx, loss))\n",
    "\n",
    "                self.add_result(value=loss.item(), name='Train_Loss', tag='Loss', counter=epoch + (batch_counter / self.train_data_loader.data_loader.num_batches))\n",
    "\n",
    "                self.clog.show_image_grid(data.float().cpu(), name=\"data\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "                self.clog.show_image_grid(target.float().cpu(), name=\"mask\", title=\"Mask\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(torch.argmax(pred.cpu(), dim=1, keepdim=True), name=\"unt_argmax\", title=\"Unet\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(pred.cpu()[:, 1:2, ], name=\"unt\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def validate(self, epoch):\n",
    "        self.elog.print('VALIDATE')\n",
    "        self.model.eval()\n",
    "\n",
    "        data = None\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_batch in self.val_data_loader:\n",
    "                data = data_batch['data'][0].float().to(self.device)\n",
    "                target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "                pred = self.model(data)\n",
    "                pred_softmax = F.softmax(pred, dim=1)  # We calculate a softmax, because our SoftDiceLoss expects that as an input. The CE-Loss does the softmax internally.\n",
    "\n",
    "                loss = self.dice_loss(pred_softmax, target.squeeze()) + self.ce_loss(pred, target.squeeze())\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, np.mean(loss_list)))\n",
    "\n",
    "        self.add_result(value=np.mean(loss_list), name='Val_Loss', tag='Loss', counter=epoch+1)\n",
    "\n",
    "        self.clog.show_image_grid(data.float().cpu(), name=\"data_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "        self.clog.show_image_grid(target.float().cpu(), name=\"mask_val\", title=\"Mask\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(torch.argmax(pred.data.cpu(), dim=1, keepdim=True), name=\"unt_argmax_val\", title=\"Unet\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(pred.data.cpu()[:, 1:2, ], name=\"unt_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "        \n",
    "def test(self):\n",
    "        # TODO\n",
    "        print('TODO: Implement your test() method here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f19a50b541a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[1;31m# visdomlogger_kwargs={\"auto_start\": c.start_visdom},\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         loggers={\n\u001b[1;32m----> 5\u001b[1;33m                             \u001b[1;34m\"visdom\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"visdom\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"auto_start\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_visdom\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                         }\n\u001b[0;32m      7\u001b[0m                         )\n",
      "\u001b[1;31mTypeError\u001b[0m: object() takes no parameters"
     ]
    }
   ],
   "source": [
    " exp = UNetExperiment(config=c, name=c.name, n_epochs=c.n_epochs,\n",
    "                         seed=42, append_rnd_to_name=c.append_rnd_string, globs=globals(),\n",
    "                         # visdomlogger_kwargs={\"auto_start\": c.start_visdom},\n",
    "                         loggers={\n",
    "                             \"visdom\": (\"visdom\", {\"auto_start\": c.start_visdom})\n",
    "                         }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = UNetExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
