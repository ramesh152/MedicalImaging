{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from batchgenerators.transforms import Compose, MirrorTransform\n",
    "from batchgenerators.transforms.crop_and_pad_transforms import CenterCropTransform, RandomCropTransform\n",
    "from batchgenerators.transforms.spatial_transforms import ResizeTransform, SpatialTransform\n",
    "from batchgenerators.transforms.utility_transforms import NumpyToTensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from trixi.util.pytorchutils import set_seed\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "\n",
    "from batchgenerators.dataloading import SlimDataLoaderBase\n",
    "\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from trixi.experiment.pytorchexperiment import PytorchExperiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., apply_nonlin=None, batch_dice=False, do_bg=True, smooth_in_nom=True, background_weight=1, rebalance_weights=None):\n",
    "        \"\"\"\n",
    "        hahaa no documentation for you today\n",
    "        :param smooth:\n",
    "        :param apply_nonlin:\n",
    "        :param batch_dice:\n",
    "        :param do_bg:\n",
    "        :param smooth_in_nom:\n",
    "        :param background_weight:\n",
    "        :param rebalance_weights:\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        if not do_bg:\n",
    "            assert background_weight == 1, \"if there is no bg, then set background weight to 1 you dummy\"\n",
    "        self.rebalance_weights = rebalance_weights\n",
    "        self.background_weight = background_weight\n",
    "        if smooth_in_nom:\n",
    "            self.smooth_in_nom = smooth\n",
    "        else:\n",
    "            self.smooth_in_nom = 0\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.y_onehot = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "        shp_x = x.shape\n",
    "        shp_y = y.shape\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            y = y.view((shp_y[0], 1, *shp_y[1:]))\n",
    "        # now x and y should have shape (B, C, X, Y(, Z))) and (B, 1, X, Y(, Z))), respectively\n",
    "        y_onehot = torch.zeros(shp_x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            y_onehot = y_onehot.cuda(x.device.index)\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "        if not self.do_bg:\n",
    "            x = x[:, 1:]\n",
    "            y_onehot = y_onehot[:, 1:]\n",
    "        if not self.batch_dice:\n",
    "            if self.background_weight != 1 or (self.rebalance_weights is not None):\n",
    "                raise NotImplementedError(\"nah son\")\n",
    "            l = soft_dice(x, y_onehot, self.smooth, self.smooth_in_nom)\n",
    "        else:\n",
    "            l = soft_dice_per_batch_2(x, y_onehot, self.smooth, self.smooth_in_nom,\n",
    "                                      background_weight=self.background_weight,\n",
    "                                      rebalance_weights=self.rebalance_weights)\n",
    "        return l\n",
    "\n",
    "\n",
    "def soft_dice_per_batch(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1):\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    weights = torch.ones(intersect.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "def soft_dice_per_batch_2(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1, rebalance_weights=None):\n",
    "    if rebalance_weights is not None and len(rebalance_weights) != gt.shape[1]:\n",
    "        rebalance_weights = rebalance_weights[1:] # this is the case when use_bg=False\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    tp = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    fn = sum_tensor((1 - net_output) * gt, axes, keepdim=False)\n",
    "    fp = sum_tensor(net_output * (1 - gt), axes, keepdim=False)\n",
    "    weights = torch.ones(tp.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    if rebalance_weights is not None:\n",
    "        rebalance_weights = torch.from_numpy(rebalance_weights).float()\n",
    "        if net_output.device.type == \"cuda\":\n",
    "            rebalance_weights = rebalance_weights.cuda(net_output.device.index)\n",
    "        tp = tp * rebalance_weights\n",
    "        fn = fn * rebalance_weights\n",
    "    result = (- ((2 * tp + smooth_in_nom) / (2 * tp + fp + fn + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "def soft_dice(net_output, gt, smooth=1., smooth_in_nom=1.):\n",
    "    axes = tuple(range(2, len(net_output.size())))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth))).mean()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, in_channels=1, initial_filter_size=64, kernel_size=3, num_downs=4, norm_layer=nn.InstanceNorm2d):\n",
    "        # norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-1), out_channels=initial_filter_size * 2 ** num_downs,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, norm_layer=norm_layer, innermost=True)\n",
    "        for i in range(1, num_downs):\n",
    "            unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-(i+1)),\n",
    "                                                 out_channels=initial_filter_size * 2 ** (num_downs-i),\n",
    "                                                 num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=in_channels, out_channels=initial_filter_size,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer,\n",
    "                                             outermost=True)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None, num_classes=1, kernel_size=3,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        # downconv\n",
    "        pool = nn.MaxPool2d(2, stride=2)\n",
    "        conv1 = self.contract(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "        conv2 = self.contract(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "\n",
    "        # upconv\n",
    "        conv3 = self.expand(in_channels=out_channels*2, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        conv4 = self.expand(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        if outermost:\n",
    "            final = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
    "            down = [conv1, conv2]\n",
    "            up = [conv3, conv4, final]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels,\n",
    "                                        kernel_size=2, stride=2)\n",
    "            model = [pool, conv1, conv2, upconv]\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels, kernel_size=2, stride=2)\n",
    "\n",
    "            down = [pool, conv1, conv2]\n",
    "            up = [conv3, conv4, upconv]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    @staticmethod\n",
    "    def contract(in_channels, out_channels, kernel_size=3, norm_layer=nn.InstanceNorm2d):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            norm_layer(out_channels),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def expand(in_channels, out_channels, kernel_size=3):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(layer, target_width, target_height):\n",
    "        batch_size, n_channels, layer_width, layer_height = layer.size()\n",
    "        xy1 = (layer_width - target_width) // 2\n",
    "        xy2 = (layer_height - target_height) // 2\n",
    "        return layer[:, :, xy1:(xy1 + target_width), xy2:(xy2 + target_height)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            crop = self.center_crop(self.model(x), x.size()[2], x.size()[3])\n",
    "            return torch.cat([x, crop], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode=\"train\", target_size=128):\n",
    "    tranform_list = []\n",
    "\n",
    "    if mode == \"train\":\n",
    "        tranform_list = [# CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=(target_size,target_size), order=1),\n",
    "                         MirrorTransform(axes=(1,)),\n",
    "                         SpatialTransform(patch_size=(target_size, target_size), random_crop=False,\n",
    "                                          patch_center_dist_from_border=target_size // 2,\n",
    "                                          do_elastic_deform=True, alpha=(0., 900.), sigma=(20., 30.),\n",
    "                                          do_rotation=True, p_rot_per_sample=0.8,\n",
    "                                          angle_x=(-15. / 360 * 2. * np.pi, 15. / 360 * 2. * np.pi), angle_y=(0, 1e-8), angle_z=(0, 1e-8),\n",
    "                                          scale=(0.85, 1.25), p_scale_per_sample=0.8,\n",
    "                                          border_mode_data=\"nearest\", border_mode_seg=\"nearest\"),\n",
    "                         ]\n",
    "\n",
    "\n",
    "    elif mode == \"val\":\n",
    "        tranform_list = [CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=target_size, order=1),\n",
    "                         ]\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        tranform_list = [CenterCropTransform(crop_size=target_size),\n",
    "                         ResizeTransform(target_size=target_size, order=1),\n",
    "                         ]\n",
    "\n",
    "    tranform_list.append(NumpyToTensor())\n",
    "\n",
    "    return Compose(tranform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.transform = transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.is_indexable = False\n",
    "        if hasattr(self.dataset, \"__getitem__\") and not (hasattr(self.dataset, \"use_next\") and self.dataset.use_next is True):\n",
    "            self.is_indexable = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if not self.is_indexable:\n",
    "            item = next(self.dataset)\n",
    "        else:\n",
    "            item = self.dataset[index]\n",
    "        item = self.transform(**item)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset.num_batches)\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader(object):\n",
    "    def __init__(self, data_loader, transform, num_processes, **kwargs):\n",
    "\n",
    "        self.cntr = 1\n",
    "        self.ds_wrapper = WrappedDataset(data_loader, transform)\n",
    "\n",
    "        self.generator = DataLoader(self.ds_wrapper, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                                    num_workers=num_processes, pin_memory=True, drop_last=False,\n",
    "                                    worker_init_fn=self.get_worker_init_fn())\n",
    "\n",
    "        self.num_processes = num_processes\n",
    "        self.iter = None\n",
    "\n",
    "    def get_worker_init_fn(self):\n",
    "        def init_fn(worker_id):\n",
    "            set_seed(worker_id + self.cntr)\n",
    "\n",
    "        return init_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.kill_iterator()\n",
    "        self.iter = iter(self.generator)\n",
    "        return self.iter\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter is None:\n",
    "            self.iter = iter(self.generator)\n",
    "        return next(self.iter)\n",
    "\n",
    "    def renew(self):\n",
    "        self.cntr += 1\n",
    "        self.kill_iterator()\n",
    "        self.generator.worker_init_fn = self.get_worker_init_fn()\n",
    "        self.iter = iter(self.generator)\n",
    "\n",
    "    def restart(self):\n",
    "        pass\n",
    "        # self.iter = iter(self.generator)\n",
    "\n",
    "    def kill_iterator(self):\n",
    "        try:\n",
    "            if self.iter is not None:\n",
    "                self.iter._shutdown_workers()\n",
    "                for p in self.iter.workers:\n",
    "                    p.terminate()\n",
    "        except:\n",
    "            print(\"Could not kill Dataloader Iterator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_dir, pattern='*.npy', slice_offset=5, keys=None):\n",
    "    fls = []\n",
    "    files_len = []\n",
    "    slices_ax = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        i = 0\n",
    "        for filename in sorted(fnmatch.filter(files, pattern)):\n",
    "\n",
    "            if keys is not None and filename[:-4] in keys:\n",
    "                npy_file = os.path.join(root, filename)\n",
    "                numpy_array = np.load(npy_file, mmap_mode=\"r\")\n",
    "\n",
    "                fls.append(npy_file)\n",
    "                files_len.append(numpy_array.shape[1])\n",
    "\n",
    "                slices_ax.extend([(i, j) for j in range(slice_offset, files_len[-1] - slice_offset)])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return fls, files_len, slices_ax,\n",
    "\n",
    "\n",
    "class NumpyDataSet(object):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000, seed=None, num_processes=8, num_cached_per_queue=8 * 4, target_size=128,\n",
    "                 file_pattern='*.npy', label_slice=1, input_slice=(0,), do_reshuffle=True, keys=None):\n",
    "\n",
    "        data_loader = NumpyDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, num_batches=num_batches, seed=seed, file_pattern=file_pattern,\n",
    "                                      input_slice=input_slice, label_slice=label_slice, keys=keys)\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.do_reshuffle = do_reshuffle\n",
    "        self.number_of_slices = 1\n",
    "\n",
    "        self.transforms = get_transforms(mode=mode, target_size=target_size)\n",
    "        self.augmenter = MultiThreadedDataLoader(data_loader, self.transforms, num_processes=num_processes,\n",
    "                                                 num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "        self.augmenter.restart()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.do_reshuffle:\n",
    "            self.data_loader.reshuffle()\n",
    "        self.augmenter.renew()\n",
    "        return self.augmenter\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.augmenter)\n",
    "\n",
    "\n",
    "class NumpyDataLoader(SlimDataLoaderBase):\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000,\n",
    "                 seed=None, file_pattern='*.npy', label_slice=1, input_slice=(0,), keys=None):\n",
    "\n",
    "        self.files, self.file_len, self.slices = load_dataset(base_dir=base_dir, pattern=file_pattern, slice_offset=0, keys=keys, )\n",
    "        super(NumpyDataLoader, self).__init__(self.slices, batch_size, num_batches)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.slice_idxs = list(range(0, len(self.slices)))\n",
    "\n",
    "        self.data_len = len(self.slices)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        if isinstance(label_slice, int):\n",
    "            label_slice = (label_slice,)\n",
    "        self.input_slice = input_slice\n",
    "        self.label_slice = label_slice\n",
    "\n",
    "        self.np_data = np.asarray(self.slices)\n",
    "\n",
    "    def reshuffle(self):\n",
    "        print(\"Reshuffle...\")\n",
    "        random.shuffle(self.slice_idxs)\n",
    "        print(\"Initializing... this might take a while...\")\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        open_arr = random.sample(self._data, self.batch_size)\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        slice_idxs = self.slice_idxs\n",
    "        data_len = len(self.slices)\n",
    "        np_data = self.np_data\n",
    "\n",
    "        if item > len(self):\n",
    "            raise StopIteration()\n",
    "        if (item * self.batch_size) == data_len:\n",
    "            raise StopIteration()\n",
    "\n",
    "        start_idx = (item * self.batch_size) % data_len\n",
    "        stop_idx = ((item + 1) * self.batch_size) % data_len\n",
    "\n",
    "        if ((item + 1) * self.batch_size) == data_len:\n",
    "            stop_idx = data_len\n",
    "\n",
    "        if stop_idx > start_idx:\n",
    "            idxs = slice_idxs[start_idx:stop_idx]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "        open_arr = np_data[idxs]\n",
    "\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def get_data_from_array(self, open_array):\n",
    "        data = []\n",
    "        fnames = []\n",
    "        slice_idxs = []\n",
    "        labels = []\n",
    "\n",
    "        for slice in open_array:\n",
    "            fn_name = self.files[slice[0]]\n",
    "\n",
    "            numpy_array = np.load(fn_name, mmap_mode=\"r\")\n",
    "\n",
    "            numpy_slice = numpy_array[ :, slice[1], ]\n",
    "            data.append(numpy_slice[None, self.input_slice[0]])   # 'None' keeps the dimension\n",
    "\n",
    "            if self.label_slice is not None:\n",
    "                labels.append(numpy_slice[None, self.label_slice[0]])   # 'None' keeps the dimension\n",
    "\n",
    "            fnames.append(self.files[slice[0]])\n",
    "            slice_idxs.append(slice[1])\n",
    "\n",
    "        ret_dict = {'data': np.asarray(data), 'fnames': fnames, 'slice_idxs': slice_idxs}\n",
    "        if self.label_slice is not None:\n",
    "            ret_dict['seg'] = np.asarray(labels)\n",
    "\n",
    "        return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNetExperiment(PytorchExperiment):\n",
    "    \"\"\"\n",
    "    The UnetExperiment is inherited from the PytorchExperiment. It implements the basic life cycle for a segmentation task with UNet(https://arxiv.org/abs/1505.04597).\n",
    "    It is optimized to work with the provided NumpyDataLoader.\n",
    "\n",
    "    The basic life cycle of a UnetExperiment is the same s PytorchExperiment:\n",
    "\n",
    "        setup()\n",
    "        (--> Automatically restore values if a previous checkpoint is given)\n",
    "        prepare()\n",
    "\n",
    "        for epoch in n_epochs:\n",
    "            train()\n",
    "            validate()\n",
    "            (--> save current checkpoint)\n",
    "\n",
    "        end()\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        pkl_dir = self.config.split_dir\n",
    "        with open(os.path.join(pkl_dir, \"splits.pkl\"), 'rb') as f:\n",
    "            splits = pickle.load(f)\n",
    "\n",
    "        tr_keys = splits[self.config.fold]['train']\n",
    "        val_keys = splits[self.config.fold]['val']\n",
    "        test_keys = splits[self.config.fold]['test']\n",
    "\n",
    "        self.device = torch.device(self.config.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.train_data_loader = NumpyDataSet(self.config.data_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                              keys=tr_keys)\n",
    "        self.val_data_loader = NumpyDataSet(self.config.data_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                            keys=val_keys, mode=\"val\", do_reshuffle=False)\n",
    "        self.test_data_loader = NumpyDataSet(self.config.data_test_dir, target_size=self.config.patch_size, batch_size=self.config.batch_size,\n",
    "                                             keys=test_keys, mode=\"test\", do_reshuffle=False)\n",
    "        self.model = UNet(num_classes=self.config.num_classes, in_channels=self.config.in_channels)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # We use a combination of DICE-loss and CE-Loss in this example.\n",
    "        # This proved good in the medical segmentation decathlon.\n",
    "        self.dice_loss = SoftDiceLoss(batch_dice=True)  # Softmax for DICE Loss!\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()  # No softmax for CE Loss -> is implemented in torch!\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # If directory for checkpoint is provided, we load it.\n",
    "        if self.config.do_load_checkpoint:\n",
    "            if self.config.checkpoint_dir == '':\n",
    "                print('checkpoint_dir is empty, please provide directory to load checkpoint.')\n",
    "            else:\n",
    "                self.load_checkpoint(name=self.config.checkpoint_dir, save_types=(\"model\"))\n",
    "\n",
    "        self.save_checkpoint(name=\"checkpoint_start\")\n",
    "        self.elog.print('Experiment set up.')\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.elog.print('=====TRAIN=====')\n",
    "        self.model.train()\n",
    "\n",
    "        data = None\n",
    "        batch_counter = 0\n",
    "        for data_batch in self.train_data_loader:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Shape of data_batch = [1, b, c, w, h]\n",
    "            # Desired shape = [b, c, w, h]\n",
    "            # Move data and target to the GPU\n",
    "            data = data_batch['data'][0].float().to(self.device)\n",
    "            target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "            pred = self.model(data)\n",
    "            pred_softmax = F.softmax(pred, dim=1)  # We calculate a softmax, because our SoftDiceLoss expects that as an input. The CE-Loss does the softmax internally.\n",
    "\n",
    "            loss = self.dice_loss(pred_softmax, target.squeeze()) + self.ce_loss(pred, target.squeeze())\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Some logging and plotting\n",
    "            if (batch_counter % self.config.plot_freq) == 0:\n",
    "                self.elog.print('Epoch: {0} Loss: {1:.4f}'.format(self._epoch_idx, loss))\n",
    "\n",
    "                self.add_result(value=loss.item(), name='Train_Loss', tag='Loss', counter=epoch + (batch_counter / self.train_data_loader.data_loader.num_batches))\n",
    "\n",
    "                self.clog.show_image_grid(data.float().cpu(), name=\"data\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "                self.clog.show_image_grid(target.float().cpu(), name=\"mask\", title=\"Mask\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(torch.argmax(pred.cpu(), dim=1, keepdim=True), name=\"unt_argmax\", title=\"Unet\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(pred.cpu()[:, 1:2, ], name=\"unt\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        self.elog.print('VALIDATE')\n",
    "        self.model.eval()\n",
    "\n",
    "        data = None\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_batch in self.val_data_loader:\n",
    "                data = data_batch['data'][0].float().to(self.device)\n",
    "                target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "                pred = self.model(data)\n",
    "                pred_softmax = F.softmax(pred, dim=1)  # We calculate a softmax, because our SoftDiceLoss expects that as an input. The CE-Loss does the softmax internally.\n",
    "\n",
    "                loss = self.dice_loss(pred_softmax, target.squeeze()) + self.ce_loss(pred, target.squeeze())\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, np.mean(loss_list)))\n",
    "\n",
    "        self.add_result(value=np.mean(loss_list), name='Val_Loss', tag='Loss', counter=epoch+1)\n",
    "\n",
    "        self.clog.show_image_grid(data.float().cpu(), name=\"data_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "        self.clog.show_image_grid(target.float().cpu(), name=\"mask_val\", title=\"Mask\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(torch.argmax(pred.data.cpu(), dim=1, keepdim=True), name=\"unt_argmax_val\", title=\"Unet\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(pred.data.cpu()[:, 1:2, ], name=\"unt_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "\n",
    "    def test(self):\n",
    "        # TODO\n",
    "        print('TODO: Implement your test() method here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trixi.util import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"base_dir\": \"E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/output_experiment\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/preprocessed\",\n",
      "    \"data_root_dir\": \"E:/IIITB/MedicalImagingDecathalon\",\n",
      "    \"data_test_dir\": \"E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/preprocessed\",\n",
      "    \"dataset_name\": \"Task04_Hippocampus\",\n",
      "    \"device\": \"cpu\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"google_drive_id\": \"1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C\",\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"n_epochs\": 1,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"split_dir\": \"E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus\",\n",
      "    \"start_visdom\": false\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n"
     ]
    }
   ],
   "source": [
    "data_root_dir = 'E:/IIITB/MedicalImagingDecathalon'  # The path where the downloaded dataset is stored.\n",
    "\n",
    "c = Config(\n",
    "        update_from_argv=True,\n",
    "\n",
    "        # Train parameters\n",
    "        num_classes=3,\n",
    "        in_channels=1,\n",
    "        batch_size=8,\n",
    "        patch_size=64,\n",
    "        n_epochs=1,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cpu\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Basic_Unet',\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,\n",
    "        start_visdom=False,\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        # Adapt to your own path, if needed.\n",
    "        google_drive_id='1RzPB1_bqzQhlWvU-YGvZzhx2omcDh38C',\n",
    "        dataset_name='Task04_Hippocampus',\n",
    "        base_dir= 'E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/output_experiment',  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir='E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/preprocessed',  # This is where your training and validation data is stored\n",
    "        data_test_dir='E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus/preprocessed',  # This is where your test data is stored\n",
    "\n",
    "        split_dir= 'E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus',  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "    )\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "WARNING:visdom:Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n",
      "ERROR:visdom:Handshake status 200 OK\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_getfullpathname: path should be string, bytes or os.PathLike, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-43961d7b0ba7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m exp = UNetExperiment(config=c, name=c.name, n_epochs=c.n_epochs,\n\u001b[0;32m      2\u001b[0m                          \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend_rnd_to_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_rnd_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                          \u001b[0mvisdomlogger_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"auto_start\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_visdom\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                          )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\experiment\\pytorchexperiment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, name, n_epochs, seed, base_dir, globs, resume, ignore_resume_config, resume_save_types, resume_reset_epochs, parse_sys_argv, parse_config_sys_argv, checkpoint_to_cpu, safe_checkpoint_every_epoch, use_visdomlogger, visdomlogger_kwargs, visdomlogger_c_freq, use_explogger, explogger_kwargs, explogger_c_freq, use_telegrammessagelogger, telegrammessagelogger_kwargs, telegrammessagelogger_c_freq, append_rnd_to_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mzip_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sources.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m             \u001b[0mSourcePacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;31m# Init objects in config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\util\\sourcepacker.py\u001b[0m in \u001b[0;36mzip_sources\u001b[1;34m(globs, filename)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mpy_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_sources_and_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mrepo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgit_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__file__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\util\\sourcepacker.py\u001b[0m in \u001b[0;36mgit_info\u001b[1;34m(file_)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mold_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\ntpath.py\u001b[0m in \u001b[0;36mabspath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;34m\"\"\"Return the absolute version of a path.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_getfullpathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_abspath_fallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _getfullpathname: path should be string, bytes or os.PathLike, not NoneType"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n"
     ]
    }
   ],
   "source": [
    "exp = UNetExperiment(config=c, name=c.name, n_epochs=c.n_epochs,\n",
    "                         seed=42, append_rnd_to_name=c.append_rnd_string, globs=globals(),\n",
    "                         visdomlogger_kwargs={\"auto_start\": c.start_visdom}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "WARNING:visdom:Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n"
     ]
    }
   ],
   "source": [
    "exp = UNetExperiment(config=c, name='unet_experiment', n_epochs=c.n_epochs,\n",
    "                         seed=42, append_rnd_to_name=c.append_rnd_string, visdomlogger_kwargs={\"auto_start\": c.start_visdom})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Sukanya\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\experiment\\experiment.py\", line 70, in run\n",
      "    self.setup()\n",
      "\n",
      "  File \"<ipython-input-7-af216acb0caf>\", line 23, in setup\n",
      "    with open(os.path.join(pkl_dir, \"splits.pkl\"), 'rb') as f:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:err-Gl0cbPlMBo:  File \"C:\\Users\\Sukanya\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\experiment\\experiment.py\", line 70, in run\n",
      "    self.setup()\n",
      "\n",
      "  File \"<ipython-input-7-af216acb0caf>\", line 23, in setup\n",
      "    with open(os.path.join(pkl_dir, \"splits.pkl\"), 'rb') as f:\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus\\\\splits.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a33583383f44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\experiment\\experiment.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%y-%m-%d_%H:%M:%S\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn_net\\lib\\site-packages\\trixi\\experiment\\experiment.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-af216acb0caf>\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mpkl_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"splits.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0msplits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/IIITB/MedicalImagingDecathalon/Task04_Hippocampus\\\\splits.pkl'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n",
      "ERROR:visdom:Handshake status 200 OK\n"
     ]
    }
   ],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
